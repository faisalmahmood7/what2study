# what2study LLM Python Service
This service is responsible for handling data files, embeddings, vector store and q&a generation with OpenAI and Ollama
You need to setup your Ollama API and enable it from reactjs frontend application in order to allow Ollama (local) LLMs usage


## How to Run
Step 1- source llmservice/bin/activate

Step 2- pip3 install -r requirements. txt

Step 3- python3 main.py

note: Install any missing libraries
langchain version should be langchain==0.2.2 and langchain-community==0.0.38


## Environment
Service running on port 5009
